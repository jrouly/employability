@application.layout.main(title = "Main") {
    <h1>About</h1>

    <p>
        <i>Employability</i> is a research project exploring how universities are preparing students for the workforce.
        Its name comes from the notion of how <i>employable</i> a student is upon graduation.
    </p>

    <h2>Contents</h2>

    <ol>
        <li><a href="#background">Background</a></li>
        <ol>
            <li><a href="#employability">Employability</a></li>
            <li><a href="#topic-modeling">Topic Modeling</a></li>
        </ol>
        <li><a href="#data-sources">Data Sources</a></li>
        <ol>
            <li><a href="#job-postings">Job Postings</a></li>
            <li><a href="#course-descriptions">Course Descriptions</a></li>
        </ol>
        <li><a href="#pre-processing">Pre Processing</a></li>
        <ol>
            <li><a href="#language-detection">Language Detection</a></li>
            <li><a href="#lowercasing">Lowercasing</a></li>
            <li><a href="#tokenization">Tokenization</a></li>
            <li><a href="#stop-word">Stop Word Removal</a></li>
            <li><a href="#stemming">Stemming</a></li>
        </ol>
    </ol>

    <h2 id="background">Background</h2>

    <h3 id="employability">Employability</h3>

    <p>
        The <em>employability</em> of a university graduate refers generally to the ability of that graduate to obtain,
        maintain, and perform in a career setting. Yorke specifically defines employability as a set of "skills,
        understandings, and personal attributes" that influence this ability. Others in the field concur with this
        definition, emphasizing that any holistic definition of employability must include academic or domain skills and
        expertise, transferrable skills, employer expectations and needs, graduate personality characteristics, and even
        external societal constraints.
    </p>

    <h3 id="topic-modeling">Topic Modeling</h3>

    <p>
        The main contribution of this work is a mechanism for the automated measure of potential employability,
        specifically the automated measure of how closely concepts taught at the postsecondary level match expected
        domain skills in the workforce. In order to compute this measure, we must first be able to automatically
        identify curricular concepts and job skills. Machine Learning (ML) and Natural Language Processing (NLP) present
        a class of methods which satisfy this need. ML is an interdisciplinary field of artificial intelligence and
        statistics which provides methods for pattern detection and approximation of unknown functions in myriad
        domains. NLP seeks to parse and understand natural human language in an automated fashion. Topic modeling is a
        class of algorithms in the intersection of these two fields which seeks to identify the topics within a corpus.
        The specific definition of a topic is highly dependent on the method used, but generally refers to a concept or
        idea present within a body of text.
    </p>

    <p>
        Latent topic models approximate topics based on underlying (latent) textual patterns. The topics are not
        defined explicitly anywhere in the text. Latent dirichlet allocation (LDA) is a topic modeling method capable
        of identifying such a latent model. In LDA topics are frequency distributions of terms over the vocabulary
        of a body of text. For example, from a collection of scientific articles about frogs, a hypothetical latent topic
        might include such terms as rainforest, amphibious, and habitat with varying associated frequency. It is
        possible to validate the performance of LDA in an automated fashion when trained on a labeled set of
        data. Extensions of LDA, such as supervised LDA (sLDA), provide a mechanism to tie the latent topics
        back to explicit labels within the data set.
    </p>

    <h2 id="data-sources">Data Sources</h2>

    <h3 id="job-postings">Job Postings</h3>

    <ul>
        <li><a href="https://data.world/promptcloud/30000-job-postings-from-seek-australia">SEEK Australia</a></li>
        <li><a href="https://data.world/promptcloud/50000-job-board-records-from-reed-uk">Reed UK</a></li>
        <li><a href="https://data.world/promptcloud/us-jobs-on-dice-com">Dice.com US</a></li>
    </ul>

    <h3 id="course-descriptions">Course Descriptions</h3>

    <ul>
        <li>TBD</li>
    </ul>

    <h2 id="pre-processing">Pre Processing</h2>

    <p>
        Human language text, specifically text sourced from the Internet, is notoriously noisy and inconsistently
        formatted. Pre-processing the data is therefore a critical stage before LDA can be performed in order to
        increase the chances of quality topic inference. Pre-processing reduces noise, removes outliers, and
        standardizes a corpus of documents which otherwise are wildly nonstandard.
    </p>

    <p>
        The steps taken in pre-processing are as follows, in order.
    </p>

    <h3 id="language-detection">Language Detection</h3>

    <p>
        Documents that are not written in English are immediately removed from the dataset. Language recognition is
        performed on the raw data using a trained model from the <a href="https://opennlp.apache.org/">Apache OpenNLP library</a>.
        Any document that is not categorized primarily as English with a confidence of at least 90% is immediately removed.
    </p>

    <h3 id="lowercasing">Lowercasing</h3>

    <p>Every document is consistently lowercased to avoid case sensitive topic inference.</p>

    <h3 id="tokenization">Tokenization</h3>

    <p>
        Text is tokenized according to a statistical model provided by <a href="https://opennlp.apache.org/">Apache OpenNLP</a>
        trained on an English language corpus. Tokens of length three or less are dropped to reduce noise.
    </p>

    <h3 id="stop-word">Tokenization</h3>

    <p>
        <em>Stop words</em> are words with little to not unique semantic value. They are words that glue together the
        meatier, more meaningful words in a language. For example, "do", "it", and "and" are all English stop words.
        Indeed, most conjunctions, pronouns, and prepositions are considered stop words. Using a list of English
        stop words from the <a href="https://www.nltk.org/">Natural Language Toolkit</a>, all tokens identified as
        stop words are removed to uniformly increase the semantic content of tokens.
    </p>

    <h3 id="stemming">Stemming</h3>

    <p>
        A standard Porter stemmer, again provided by <a href="https://opennlp.apache.org/">Apache OpenNLP</a>, is
        applied to the tokens. The stemmer reduces English language words to their "stem" or root words. For example,
        "salaried", "salaries", and "salary" might all be reduced to "salari". In this way, words with shared semantics
        that would otherwise be unique tokens with low frequency are made to be significantly higher frequency.
    </p>

}
