\section{Methods}

This work can be split into four stages: data collection, data cleaning, topic modeling, and analysis.

\subsection{Data collection}

Large volumes of human language text must be acquired prior to beginning the analysis.
This text falls into two categories: career data and curricular data.
The Internet was used as the primary source for all data acquired.

\subsubsection{Career data}

Career data is any human language text describing the expected skills and qualifications of an ideal candidate.
This kind of data is present in many formats.
A contract signed between an employer and employee may contain this data in a description of day to day roles and responsibilities.
Corporate publications (blog posts, articles) may also contain descriptions of employee behavior and roles.
However, perhaps the most straightforward source of data for this corpus is online job postings.
Job postings generally follow a predictable format: logistics (role, seniority, location), brief company profile, and then a list of responsibilities and expected prerequisite skills.
Logistic information is an ample source of noise, but company descriptions and more so responsibilities and prerequisites are exactly the kind of human language career data necessary for this analysis.
Open data sources were identified on the Internet and aggregated into a corpus of over 92,000 job description documents.~\cite{data.world:promptcloud}


\subsubsection{Curricular data}

Curricular data is human language text that describes expected outcomes of postsecondary courses.
This data set must contain the necessary data to identify academic core skills addressed at the postsecondary level.
This is much more subtle and difficult to standardize proposition than career data.
A course curriculum contains exactly this data, but the formatting and availability of course curriculum documents vary widely and thus they do not lend themselves toward a broadly scoped automated data ingestion process.

Perhaps the most common source of curricular data is the course syllabus.
Syllabi contain similar information in a much more concise format.
Additionally, they tend to communicate expectations of students, course logistics, and the end goals of the course curriculum.
Several projects to collect corpora of syllabi exist, perhaps most notable being the Open Syllabus Project (OSP), but in general the raw data is not easily accessed.

Another ubiquitous curricular data modality is the course description.
These documents are even more concise than syllabi, but are collected in ``catalogs'' for the express purpose of public consumption.
This makes course descriptions an ideal target for data collection.
Additionally, previous work has expressed success building web scrapers to extract course data from publicly available websites.~\cite{rouly2015}
The present work involved the creation of web scrapers to ingest a course description dataset of almost 20,000 course descriptions from various American universities.
A detailed breakdown of sources is available online at \href{http://employability.rouly.net}{employability.rouly.net}.

\subsection{Data cleaning}

Human language text in general contains a large amount of noise.
Free form text retrieved from myriad Internet sources even more so.
To improve on the signal to noise ratio, data was streamed through a cleaning process prior to storage in a central database.
Statistical methods were utilized to determine language content of each document, and non-English documents were filtered out.
The remaining documents were normalized via lowercasing and character sequence reduction, tokenized using a statistical model trained on English documents, and stemmed to word roots with the Porter stemmer.
Known English ``stop words'' (glue words with little distinct semantic value) were filtered out of documents.
Excessively short documents and tokens were dropped as well.
By normalizing and stemming the documents, we provide the automated Topic Modeling methods a fighting chance to identify recurrent terms.

\subsection{Topic modeling}

Using the Spark library implementations, documents were vectorized from the raw ``bag of terms'' representation using a TF-IDF measure.
These vectorized representations were fed into an LDA processor under a variety of constraint settings.
Results are reported in the next section.

\subsection{Analysis}

After modeling the corpora of documents, a measure of overlap between the two was computed.
The guiding notion in this analysis is that the proportional frequency with which topics are expressed in a corpus represents the prevalence, or importance, of that topic to the corpus.
In other words, the more frequently a topic comes up, the more relevant it is as a whole to the corpus.
Elasticsearch was used to perform an aggregation over all the modeled documents.
{\color{red}TODO.}
